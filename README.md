<div align="center">
  <h1 align="center">awesome-llm-papers</h1>
  <img src="./logo.png" alt="Logo" width="300" height="300" />
  <p>
      <a href="https://github.com/InfiniteAICreations/awesome-llm-papers">English</a> | <a href="https://github.com/InfiniteAICreations/awesome-llm-papers/blob/main/README.zh_CN.md">ç®€ä½“ä¸­æ–‡</a>
  </p>

  <h4 align="center">
    <a href="https://awesome.re">
      <img src="https://awesome.re/badge.svg" alt="Awesome" />
    </a>
    <a href="http://makeapullrequest.com">
      <img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="http://makeapullrequest.com" />
    </a>
  </h4>
</div>

ðŸ˜Ž Awesome lists about all kinds of LLM related papers

## Text / Information / Knowledge
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401): This paper introduces a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation.

## Image
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239):  This paper introduces a new class of generative models called denoising diffusion probabilistic models (DDPMs).
- [Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs](https://arxiv.org/pdf/2404.05719.pdf): Apple's paper on multimodal LLMs for mobile UI understanding.

## Architecture
- [Attention is All You Need](https://arxiv.org/abs/1706.03762): This paper introduces the Transformer architecture, which is based on the multi-head attention mechanism.
- [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/abs/2404.07143): This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation.

## Survey
- [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)
- [A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?](https://arxiv.org/abs/2303.11717)
- [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/abs/2302.09419)
- [Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers](https://arxiv.org/abs/2404.04925)
- [A Comprehensive Overview of Large Language Models](https://arxiv.org/abs/2307.06435)

## Operating System
- [AIOS: LLM Agent Operating System](https://arxiv.org/abs/2403.16971)

## Text To SQL
- [Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task](https://arxiv.org/abs/1809.08887)
- [Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning](https://arxiv.org/abs/1709.00103)
- [DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction](https://arxiv.org/abs/2304.11015)
- [KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers](https://arxiv.org/abs/2106.11455)
- [Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs](https://arxiv.org/pdf/2305.03111.pdf)

## LLMs
- [CodeGemma: Open Code Models Based on Gemma](https://storage.googleapis.com/deepmind-media/gemma/codegemma_report.pdf)
- [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/pdf/2404.14219.pdf)
- [Code Llama: Open Foundation Models for Code](https://arxiv.org/pdf/2308.12950.pdf)

## LLM fine-tuning
- [Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255)

## Security
- [LLM Agents can Autonomously Exploit One-day Vulnerabilities](https://arxiv.org/abs/2404.08144v2)

## Agents
- [AutoDev: Automated AI-Driven Development](https://arxiv.org/pdf/2403.08299.pdf)

